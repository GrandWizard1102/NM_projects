{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },  
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2e730966ec6a49e6805d7672934bb831",
            "76360ac530764910ad4cf2ffc21dcc2f",
            "26f252c4a27a45b986b9c9e047e9cf6a",
            "8dd3a96a01f54fc0aac1f1f37bbb9c75",
            "9d219a3db0d44dc18343af51b03c6203",
            "fa5fb476645545e09b8ff133b117875b",
            "2c703f9472cd48c1a3b16f9b49d1767c",
            "9955f9949fea4b2993e9d3dec463f331",
            "0e62dae145a14747aa7661cfd81eded9",
            "6a106b4948db49e48c99de3b8e88d117",
            "3842705cca6b453fa85cb8f66febe3a2",
            "88e5ffe987b44257894aadaf9ab4e554",
            "a444f4cb84f040c0a2615fce37287721",
            "c1d03ae738374f6f82b354db143e9759",
            "3a93e6db558d4f07a94e5b40671e23b5",
            "65a45e3c9fa246ea86e91bbcc28b1bf5",
            "65cbf86adc7e4304a2f1f75e3f03bf80",
            "2ca3e2cc0f1b4de0b3cf5e60e94b2448",
            "454644a616dc46b1ae468033421662fc",
            "744d1d93b3ea443a903d4d2df033f56e",
            "857ca09f46a7440c99cb5f1d8a27833a",
            "70dc306858dc47c1914d4891158dbfbb",
            "1d2925948a1a4a5991f2c542b5cd26bd",
            "1ad5b308eee24c64a6e09b6014cc3cd3",
            "390c66221a964aa18f52299f75ea538e",
            "b60e70d0f9b14d4296457f5932fae24d",
            "c9488d3d35ae4e7cbd4570ff70c6569f",
            "5843b95574dc4cc4ab2bdf5b1580e848",
            "4005cfaaf8ce443082627fc3357a900f",
            "60707402cbf8456f9c2d1189c946a943",
            "5f850e7ea92d4cf5a34b566b8c9cafce",
            "1f72698f56ce48aebfcaac9c61d0d455",
            "891162fedae0473480cec3c0854ffdc8",
            "2992c52061c34312a94b9010870ef443",
            "99790bb7616643b48ecdd6fc1fb0ee7a",
            "9137723514b34d1eb3129cf5490eedd4",
            "d4c158f1f336432e82de1ddbf0d59f77",
            "f2469d65b2b84f5a836f13d17feeded9",
            "cee8efc5ebc142fd89f6c5da5e7cf4c5",
            "08b5e6258cc548d492d2fa4a91554c0b",
            "c3baabf2180e43da90aa5ee59f1b6cf7",
            "28c9f24d6465445abe2ca460b3712528",
            "852e2e24e69a44d1a236daaed84a43d9",
            "5666e1d082634065bf6f84bd695b8e79",
            "ba845f5c75d44b26a2b7c20ceb1b57bf",
            "e07d44c7597945968f21a4d4e465fd70",
            "7749973e37114fab930b817920971184",
            "f597c7ff945b4246992bcbc743da1adf",
            "e5d5f6992565472791cdec7e87b16a65",
            "706961e606c54e75b13a10acbbfb9f84",
            "51f93f103a5c4eb4af3ac476f7edbb67",
            "1dad136aae5246f389ecb5fc85c39fe9",
            "81b4f56d573b46c5bf45f89242b61884",
            "827a691e2fb54bc4b039ae8e576e8765",
            "8328b9817f8445cd9a35f36786b0bd9a",
            "32935d7dcbe840c4b6b64a1899c591bc",
            "7addbbd647f24659a9dc4b56ed871eb7",
            "c5037bef59514c87a51b86f80423edff",
            "c07cf46922d44745a0b5080c38b0fb37",
            "e158bfc5ff424ff2bc86d0d465eed833",
            "65544a8bffe6455188707af41b887923",
            "a0e5fdf926334b3ebb11c2148ed34f24",
            "eac682f06a574d6a8d43a18e16e5cd63",
            "0f4257c2b2024bed837ccddcac3abb8f",
            "865f7354d492435ca781e6041540a131",
            "c0480d47618c443b9916c78d4081ea96",
            "0fead96e0c1445f4a608b64ad8e136f7",
            "63ed755fe90249e996c3e7604852a06e",
            "a1f503fbce344d9abef5932872ba733b",
            "101810fb04fe436c908beee4853facbc",
            "35f1d171ff104572b765dae8f95246a4",
            "80ee3ef881ff45df8a28471ea90f9f53",
            "9da1e94ffda644ccacbc8daa3f445338",
            "a6a8ce0601e942868fcd5d4d66183396",
            "f762d96d05714526ade52a1edfd810df",
            "260ee2d32bb84e268cf5850b3e876e12",
            "ab9fdfe552714c909798a37da154d008"
          ]
        },
        "id": "4tTZLLSlP08P",
        "outputId": "4247222d-36a9-4d8a-8c35-a34b3271226b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e730966ec6a49e6805d7672934bb831"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88e5ffe987b44257894aadaf9ab4e554"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d2925948a1a4a5991f2c542b5cd26bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2992c52061c34312a94b9010870ef443"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba845f5c75d44b26a2b7c20ceb1b57bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32935d7dcbe840c4b6b64a1899c591bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fead96e0c1445f4a608b64ad8e136f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Story continuation #1 ---\n",
            "Once upon a time in a distant kingdom, there was a young princess who dreamed of adventure. One day, she decided to leave the castle and explore the mysterious forest nearby.Her adventure started in the middle of the forest and ended at a mysterious village.\n",
            "\n",
            "This story is about a young princess who has never left the castle in her life. When she goes to explore the forest, she finds that there is a mysterious and powerful demon named the King. The King is the strongest demon in the world, and his presence causes many strange things to happen. The Princess is left to explore the land alone, hoping that she can find the key to defeat him.\n",
            "\n",
            "This story will be a continuation of the story I started in the first chapter. It will be more lighthearted, but still a fun read.\n",
            "\n",
            "--- Story continuation #2 ---\n",
            "Once upon a time in a distant kingdom, there was a young princess who dreamed of adventure. One day, she decided to leave the castle and explore the mysterious forest nearby.As she walked through the forest, she heard a familiar voice behind her.\n",
            "\n",
            "\"What's that? What is that?\"\n",
            "\n",
            "The princess was surprised at that voice and looked up. The voice was a familiar voice.\n",
            "\n",
            "\"It's a messenger.\"\n",
            "\n",
            "The princess was startled for a moment. The messenger was called Gaius. The messenger's name was Orestes.\n",
            "\n",
            "Gaius: \"Who's Orestes?\"\n",
            "\n",
            "Orestes: \"The messenger from my past.\"\n",
            "\n",
            "\"Orestes is from a very far country. I know nothing about it, but I know he will help you find my grandfather, who is still alive.\"\n",
            "\n",
            "\"Gaius, I've heard of a boy named Gaius, but… you know, you're not even a knight.\"\n",
            "\n",
            "\"Don't worry, I'm not that kind of person.\"\n",
            "\n",
            "\"But I think he has a lot of strength in him.\"\n",
            "\n",
            "\"Then you will help me find his grandfather?\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch --quiet\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Step 3: Load pre-trained GPT-2 tokenizer and model\n",
        "model_name = \"gpt2-medium\"  # You can use \"gpt2\", \"gpt2-medium\", \"gpt2-large\" or \"gpt2-xl\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Put model in evaluation mode and move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Step 4: Define a function to generate story continuation\n",
        "def generate_story(prompt, max_length=200, temperature=0.7, top_k=50, top_p=0.95, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generate story continuation using GPT-2.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The initial text prompt to start the story.\n",
        "        max_length (int): Maximum length of generated text including prompt.\n",
        "        temperature (float): Controls randomness in generation (higher = more random).\n",
        "        top_k (int): Limits sampling to top_k tokens.\n",
        "        top_p (float): Nucleus sampling probability threshold.\n",
        "        num_return_sequences (int): Number of generated sequences to return.\n",
        "\n",
        "    Returns:\n",
        "        List of generated story continuations.\n",
        "    \"\"\"\n",
        "    # Encode input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output sequences\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and return generated texts\n",
        "    generated_stories = []\n",
        "    for generated_sequence in output_sequences:\n",
        "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
        "        # Remove the prompt from the output to get only the continuation\n",
        "        continuation = text[len(prompt):].strip()\n",
        "        generated_stories.append(continuation)\n",
        "\n",
        "    return generated_stories\n",
        "\n",
        "# Step 5: Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    prompt = (\n",
        "        \"Once upon a time in a distant kingdom, there was a young princess who dreamed of adventure. \"\n",
        "        \"One day, she decided to leave the castle and explore the mysterious forest nearby.\"\n",
        "    )\n",
        "    stories = generate_story(prompt, max_length=250, temperature=0.8, top_k=40, top_p=0.9, num_return_sequences=2)\n",
        "\n",
        "    for i, story in enumerate(stories, 1):\n",
        "        print(f\"--- Story continuation #{i} ---\\n{prompt}{story}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade nbconvert nbformat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iWh_NEeTfCz",
        "outputId": "6085c55a-500a-4c77-8c7a-db0e4baa96c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (7.16.6)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (5.10.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (2.18.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.7.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat) (4.23.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert) (4.3.7)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert) (4.13.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install dependencies (if needed)\n",
        "!pip install transformers torch gradio --quiet\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import gradio as gr\n",
        "\n",
        "# Step 3: Load pre-trained GPT-2 tokenizer and model\n",
        "model_name = \"gpt2-medium\"  # You can use \"gpt2\", \"gpt2-medium\", \"gpt2-large\" or \"gpt2-xl\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Put model in evaluation mode and move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Step 4: Define a function to generate story continuation\n",
        "def generate_story(prompt, max_length=200, temperature=0.7, top_k=50, top_p=0.95, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generate story continuation using GPT-2.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The initial text prompt to start the story.\n",
        "        max_length (int): Maximum length of generated text including prompt.\n",
        "        temperature (float): Controls randomness in generation (higher = more random).\n",
        "        top_k (int): Limits sampling to top_k tokens.\n",
        "        top_p (float): Nucleus sampling probability threshold.\n",
        "        num_return_sequences (int): Number of generated sequences to return.\n",
        "\n",
        "    Returns:\n",
        "        List of generated story continuations.\n",
        "    \"\"\"\n",
        "    # Encode input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output sequences\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and return generated texts\n",
        "    generated_stories = []\n",
        "    for generated_sequence in output_sequences:\n",
        "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
        "        # Remove the prompt from the output to get only the continuation\n",
        "        continuation = text[len(prompt):].strip()\n",
        "        generated_stories.append(continuation)\n",
        "\n",
        "    return generated_stories\n",
        "\n",
        "# Step 5: Define Gradio interface\n",
        "def gradio_interface(prompt):\n",
        "    stories = generate_story(prompt, max_length=250, temperature=0.8, top_k=40, top_p=0.9, num_return_sequences=2)\n",
        "    return \"\\n\\n\".join([f\"Story {i+1}: {story}\" for i, story in enumerate(stories)])\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(label=\"Enter a prompt\", lines=4, placeholder=\"Once upon a time...\"),\n",
        "    outputs=\"text\",\n",
        "    live=True,\n",
        "    title=\"GPT-2 Story Generator\",\n",
        "    description=\"Enter a prompt and watch GPT-2 generate the continuation of the story.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "6AlWUBIaP1zd",
        "outputId": "f504c79d-dc7a-4fc5-b557-fe267945074e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://00eea9f38135a24ea7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://00eea9f38135a24ea7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwPkAmeBTzlY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}